{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76161f68",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f404d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4a2f",
   "metadata": {},
   "source": [
    "Parse the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774d0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qrel(doc):\n",
    "    rels = {}\n",
    "    with open(doc, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            query, iteration, document, relevancy = line.split()\n",
    "            if int(query) not in rels:\n",
    "                rels[int(query)] = {document: int(relevancy)}\n",
    "            else:\n",
    "                rels[int(query)][document] = int(relevancy)\n",
    "    return rels\n",
    "\n",
    "def parse_results(doc, rels, n_hits):\n",
    "    with open(doc, 'r') as f:\n",
    "        current_query, _, document, rank, _, _ = next(f).split()\n",
    "        if int(current_query) in rels:\n",
    "            if document not in rels[int(current_query)]:\n",
    "                rels[int(current_query)][document] = 0\n",
    "\n",
    "        rank_label_list = [(int(rank), rels[int(current_query)][document])]\n",
    "        sorted_labels = []\n",
    "        for line in f:\n",
    "            query, _, document, rank, _, _ = line.split()\n",
    "            if int(rank) <= n_hits:\n",
    "                if int(query) != int(current_query):\n",
    "                    sorted_labels = [x[1] for x in sorted(rank_label_list)]\n",
    "                    yield np.array(sorted_labels, dtype=np.int32)\n",
    "\n",
    "                    current_query = query\n",
    "                    rank_label_list = []\n",
    "\n",
    "                if int(current_query) in rels:\n",
    "                    if document in rels[int(current_query)]:\n",
    "                        rank_label_list.append((int(rank), rels[int(current_query)][document]))\n",
    "    sorted_labels = [x[1] for x in sorted(rank_label_list)]\n",
    "    return np.array(sorted_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec7113",
   "metadata": {},
   "source": [
    "Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d578ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qrl, k_sizes=[1, 5, 10, 25]):\n",
    "    qrl = [i for i in qrl if len(i) != 0]\n",
    "    results = {}\n",
    "    for k in k_sizes:\n",
    "        results['p'+str(k)] = np.mean([precision(i, k) for i in qrl])\n",
    "        results['r'+str(k)] = np.mean([recall(i, k) for i in qrl])\n",
    "        results['f'+str(k)] = np.mean([F_score(i, k) for i in qrl])\n",
    "        results['d'+str(k)] = np.mean([DCG(i, k) for i in qrl])\n",
    "        results['n'+str(k)] = np.mean([NDCG(i, k) for i in qrl])\n",
    "    results['map'] = np.mean([MAP(i) for i in qrl])\n",
    "    results['mrr'] = np.mean([MRR(i) for i in qrl])\n",
    "\n",
    "    return results\n",
    "\n",
    "def precision(query_relevancy_labels, k):\n",
    "    score = 0\n",
    "    for i, relevance in enumerate(query_relevancy_labels, 1):\n",
    "        if i <= k: score += relevance\n",
    "    if i<k: score += 0*(k-i)\n",
    "    return score/k\n",
    "\n",
    "def recall(query_relevancy_labels, k):\n",
    "    score = 0\n",
    "    doc = 0\n",
    "    for i, relevance in enumerate(query_relevancy_labels, 1):\n",
    "        if i <= k: score += relevance\n",
    "        doc += relevance\n",
    "    \n",
    "    if doc == 0: return 0\n",
    "    if i<k: score += 0*(k-i)\n",
    "    \n",
    "    return score/doc\n",
    "\n",
    "def F_score(query_relevancy_labels, k):\n",
    "    p = precision(query_relevancy_labels, k)\n",
    "    r = recall(query_relevancy_labels, k)\n",
    "    \n",
    "    if p == 0 and r == 0: return 0\n",
    "    return (2*p*r)/(p+r)\n",
    "\n",
    "def DCG(query_relevancy_labels, k):\n",
    "    # Use log with base 2\n",
    "    score = 0\n",
    "    for i, relevance in enumerate(query_relevancy_labels, 1):\n",
    "        if i <= k: score += ((2**relevance)-1)/(np.log2(1+i))\n",
    "    return score\n",
    "\n",
    "def NDCG(query_relevancy_labels, k):\n",
    "    score = DCG(query_relevancy_labels, k)\n",
    "    \n",
    "    max_qrl = [i for i in query_relevancy_labels if i!=0]\n",
    "    max_score = DCG(max_qrl, k)\n",
    "    \n",
    "    if max_score == 0: return 0\n",
    "    return score/max_score\n",
    "\n",
    "def MAP(query_relevancy_labels):\n",
    "    score = 0\n",
    "    doc = 0\n",
    "    for i, relevance in enumerate(query_relevancy_labels, 1):\n",
    "        score += relevance*precision(query_relevancy_labels, i)\n",
    "        doc += relevance\n",
    "    if doc == 0: return 0\n",
    "    return score/doc\n",
    "\n",
    "def MRR(query_relevancy_labels):\n",
    "    score = 0\n",
    "    for i, relevance in enumerate(query_relevancy_labels, 1):\n",
    "        if relevance == 1 and (i/relevance < score or score == 0):\n",
    "            score = i/relevance\n",
    "    if score == 0: return 0\n",
    "    return 1/score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b0bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_results(res_dir='./output/results', out_dir='./output/evaluations2', qrel_doc='../test_documents/qrels.robust2004.txt', k_sizes=[1, 5, 10, 25], n_hits=100):\n",
    "    rels = parse_qrel(qrel_doc)\n",
    "    \n",
    "    for doc in os.scandir(res_dir):\n",
    "        res_doc = os.path.join(res_dir, doc.name)\n",
    "        sorted_labels = [i for i in parse_results(res_doc, rels, n_hits)]\n",
    "        metrics = evaluate(sorted_labels, k_sizes)\n",
    "        \n",
    "        res_doc = os.path.join(out_dir, doc.name)\n",
    "        with open(res_doc, 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "    \n",
    "evaluate_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391df21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
